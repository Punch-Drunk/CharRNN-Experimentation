{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50d2274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42ed17f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text, seq_len, char2idx):\n",
    "        self.text = text\n",
    "        self.seq_len = seq_len\n",
    "        self.char2idx = char2idx\n",
    "        self.vocab_size = len(char2idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.seq_len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input_seq = self.text[index:index + self.seq_len]\n",
    "        target_seq = self.text[index+  1:index + self.seq_len +1]\n",
    "        input_ids = torch.tensor([self.char2idx[c] for c in input_seq], dtype=torch.long)\n",
    "        target_ids = torch.tensor([self.char2idx[c] for c in target_seq], dtype=torch.long)\n",
    "        return input_ids,target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99377857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, dropout=0.3,batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        # x: (batch, seq_len)\n",
    "        x = self.embed(x)            # (batch, seq_len, embed_size)\n",
    "        out, hidden = self.lstm(x, hidden)  # out: (batch, seq_len, hidden)\n",
    "        out = self.fc(out)           # (batch, seq_len, vocab_size)\n",
    "        log_probs = F.log_softmax(out, dim=-1) # log probs\n",
    "        return log_probs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b74f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params\n",
    "seq_len = 100\n",
    "batch_size = 1024\n",
    "epochs = 20\n",
    "hidden_size = 512\n",
    "num_layers = 4\n",
    "embed_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48ce2a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 3201634, Unique chars: 105\n"
     ]
    }
   ],
   "source": [
    "with open('war&peace.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Create character-to-index mappings\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "char2idx = { ch:i for i,ch in enumerate(chars) }\n",
    "idx2char = { i:ch for i,ch in enumerate(chars) }\n",
    "print(f\"Text length: {len(text)}, Unique chars: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "412d099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CharDataset(text, seq_len, char2idx)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CharLSTM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=7e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0dd9dea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup Batch:   0%|          | 0/3126 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup loss: 4.650972366333008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "for inputs, targets in tqdm(dataloader, desc=\"Warmup Batch\"):\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    with torch.no_grad():  # Add this - prevents gradient computation\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            log_probs, _ = model(inputs)\n",
    "            loss = criterion(log_probs.view(-1, vocab_size), targets.view(-1))\n",
    "    \n",
    "    print(f\"Warmup loss: {loss.item()}\")\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1442a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_str, length=200):\n",
    "    model.eval()\n",
    "    input_idxs = [char2idx[ch] for ch in start_str]\n",
    "    input_tensor = torch.tensor(input_idxs, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    hidden = None\n",
    "    output_text = start_str\n",
    "    for _ in range(length):\n",
    "        out, hidden = model(input_tensor, hidden)\n",
    "        # Take the last timestep's output\n",
    "        last_logits = out[0, -1, :].cpu().detach().numpy()\n",
    "        # Sample from the softmax distribution\n",
    "        probs = np.exp(last_logits - np.max(last_logits))\n",
    "        probs /= probs.sum()\n",
    "        next_idx = np.random.choice(range(vocab_size), p=probs)\n",
    "        output_text += idx2char[next_idx]\n",
    "        input_tensor = torch.tensor([[next_idx]], dtype=torch.long, device=device)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e9ce3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_during_train(model, char_to_idx, idx_to_char, seed_text=\"The\", length=200, temperature=1.0):\n",
    "    # Remember the current training state\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Initialize hidden state\n",
    "        h_0 = torch.zeros(num_layers, 1, hidden_size).to(device)\n",
    "        c_0 = torch.zeros(num_layers, 1, hidden_size).to(device)\n",
    "        hidden = (h_0, c_0)\n",
    "        \n",
    "        # Convert seed text to indices\n",
    "        input_seq = [char_to_idx.get(c, 0) for c in seed_text]\n",
    "        generated = seed_text\n",
    "        \n",
    "        # Process seed text through the model\n",
    "        for char_idx in input_seq[:-1]:\n",
    "            input_tensor = torch.tensor([[char_idx]]).to(device)\n",
    "            _, hidden = model(input_tensor, hidden)\n",
    "        \n",
    "        # Generate new characters\n",
    "        input_tensor = torch.tensor([[input_seq[-1]]]).to(device)\n",
    "        \n",
    "        for _ in range(length):\n",
    "            log_probs, hidden = model(input_tensor, hidden)\n",
    "            probs = torch.softmax(log_probs.squeeze() / temperature, dim=0)\n",
    "            next_char_idx = torch.multinomial(probs, 1).item()\n",
    "            next_char = idx_to_char[next_char_idx]\n",
    "            generated += next_char\n",
    "            input_tensor = torch.tensor([[next_char_idx]]).to(device)\n",
    "    \n",
    "    # Restore the original training state\n",
    "    model.train(was_training)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8055ff70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am learning to speak!vôAJ7qâ2ë6úç2çiksé=)ÀaNy;á‘àæ:Y’ýöbýöz713éB—dákd6ghPcO4\n",
      "7pá”YVâSgëâöckB(‘ú‘Uç8Fhïq2 hi-sï9P*njóÉb07œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 3126/3126 [15:00<00:00,  3.47it/s, loss=1.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Avg Loss: 1.5607\n",
      "I am learning to speak!” said\n",
      "Pierre.\n",
      "\n",
      "“The died travel and think and pluy because he is so heard and suddenly at the clear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 3126/3126 [14:57<00:00,  3.48it/s, loss=1.01] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Avg Loss: 1.0700\n",
      "I am learning to speak!” From the table,\n",
      "with which he adjusted he gave her “Uncle”” whispered the officer,\n",
      "entering the st\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 3126/3126 [14:58<00:00,  3.48it/s, loss=0.924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Avg Loss: 0.9570\n",
      "I am learning to speak!”\n",
      "\n",
      "He indicated the firm guardship, who had no idea by Nicholas grew\n",
      "more more and more secondly, wh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 3126/3126 [14:57<00:00,  3.48it/s, loss=0.87] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Avg Loss: 0.8875\n",
      "I am learning to speak!” Natásha did not suppose he never obliged to\n",
      "speak to him handing it down to take leave of the hut.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 3126/3126 [14:42<00:00,  3.54it/s, loss=0.87] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Avg Loss: 0.8671\n",
      "I am learning to speak!\n",
      "This world should not have to bring the Rostóvs,” said Mortemart, “but\n",
      "tell me, I’ll tell you what \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 3126/3126 [14:54<00:00,  3.49it/s, loss=0.863]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Avg Loss: 0.8671\n",
      "I am learning to speak!” said Pierre. (He was lying in the\n",
      "room till with which Pierre sat on the back of his chest and pit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 3126/3126 [14:44<00:00,  3.54it/s, loss=0.867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Avg Loss: 0.8671\n",
      "I am learning to speak!”\n",
      "\n",
      "Then called Dáníchs through the doorway.\n",
      "\n",
      "“Ah!” exclaimed Prince Andrew as if inquiring those dis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 3126/3126 [14:44<00:00,  3.53it/s, loss=0.866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Avg Loss: 0.8671\n",
      "I am learning to speak!”\n",
      "\n",
      "Dólokhov smiled his hand jauntily and nuskey. “I ought to find out\n",
      "where you’ve shot at each othe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 3126/3126 [14:44<00:00,  3.53it/s, loss=0.863]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Avg Loss: 0.8670\n",
      "I am learning to speak!” thought Nicholas, “arranging myself alone\n",
      "in the roging-horse?” said he angrily and, pointing to t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 3126/3126 [14:44<00:00,  3.53it/s, loss=0.869]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Avg Loss: 0.8671\n",
      "I am learning to speak!”\n",
      "\n",
      "“Well, now you, she’s frienda, thanks to your houses, it’s burned.\n",
      "What has been coming so that w\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 3126/3126 [14:58<00:00,  3.48it/s, loss=0.862]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Avg Loss: 0.8671\n",
      "I am learning to speak!”\n",
      "\n",
      "“You, brothers? What lives were the more very bad and I do not go away\n",
      "and only distinguish us.”\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 3126/3126 [16:21<00:00,  3.19it/s, loss=0.87] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Avg Loss: 0.8670\n",
      "I am learning to speak!” Natásha began, uped to the\n",
      "sounds in front of the cook, which were dreaving outside, was vexed by\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 3126/3126 [16:20<00:00,  3.19it/s, loss=0.866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Avg Loss: 0.8671\n",
      "I am learning to speak!” said the count. “And what marriage\n",
      "is the more former spirits of Berg and Willarski counter? My wi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 3126/3126 [16:23<00:00,  3.18it/s, loss=0.866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Avg Loss: 0.8671\n",
      "I am learning to speak!”\n",
      "\n",
      "“I’m not displeased that your Serene Highness wish,” he continued,\n",
      "“I beg you to consider that yo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 3126/3126 [16:22<00:00,  3.18it/s, loss=0.875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Avg Loss: 0.8671\n",
      "I am learning to speak!”\n",
      "\n",
      "“But what of I?” reproached him of the prince, “which I should ask\n",
      "for Bald Hills ones, and no wi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 3126/3126 [16:23<00:00,  3.18it/s, loss=0.867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Avg Loss: 0.8671\n",
      "I am learning to speak!” And Anmay,\n",
      "smiling rapidly and searchingly and confused.\n",
      "\n",
      "\n",
      "Meanwhile did not real the position of \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 3126/3126 [16:23<00:00,  3.18it/s, loss=0.867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Avg Loss: 0.8670\n",
      "I am learning to speak!” said he. “Womenín, my dear\n",
      "sir, our girls have not sent up the field, so that’s war!” thought he.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 3126/3126 [16:23<00:00,  3.18it/s, loss=0.875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Avg Loss: 0.8671\n",
      "I am learning to speak!”\n",
      "\n",
      "“Well, and how did you fail to be written with such surjotical, enlightenment\n",
      "and catches? But if\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 3126/3126 [16:22<00:00,  3.18it/s, loss=0.866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Avg Loss: 0.8671\n",
      "I am learning to speak!” he thought.\n",
      "\n",
      "Again pleased by a stone calm and appearance Frenchmen convering so calmly\n",
      "of the duf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 3126/3126 [16:22<00:00,  3.18it/s, loss=0.865]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Avg Loss: 0.8671\n",
      "I am learning to speak! I will tell you, my dear boy—I don’t\n",
      "understand that something was in somebody!” said Nicholas. “Th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_text_during_train(model, char2idx, idx2char, \"I am learning to speak!\",100))\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for inputs, targets in progress_bar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        h_0 = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "        c_0 = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "        hidden = (h_0, c_0)\n",
    "        \n",
    "        with torch.amp.autocast('cuda'):\n",
    "            log_probs, _ = model(inputs, hidden)\n",
    "            loss = criterion(log_probs.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}\")\n",
    "    print(generate_text_during_train(model, char2idx, idx2char, \"I am learning to speak!\",100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f169ab80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated text:\n",
      " My opinion on cats is outside with us; then he is considered an\n",
      "annihilatement of the merits of the and, and it was not honest in Speránski\n",
      "himported the Preobrazhénsk battalion but had already vanished.\n",
      "\n",
      "“I can’t bear the carriage,” said he.\n",
      "\n",
      "“To leave off,” said the count, crushing either from under her\n",
      "brows and looked with animation. At last, Pétya and Borís, who\n",
      "ever shrugged his steps.\n",
      "\n",
      "“Only fell from behind our knappens,” the prince struck a braker on\n",
      "his knees as if trying to make, go on, gave his horse to \n"
     ]
    }
   ],
   "source": [
    "seed = \"My opinion on cats is\"\n",
    "sample_text = generate_text(model, seed, length=500)\n",
    "print(\"Sample generated text:\\n\", sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "130b53f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup Batch:   0%|          | 0/3126 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup loss: 4.657007217407227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 6252/6252 [05:19<00:00, 19.57it/s, loss=1.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Avg Loss: 3.2781\n",
      "I am learning to speak!” “It’s not a beginned to chance the restage! But I say that what I can’t have was up this\n",
      "down to s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 6252/6252 [05:04<00:00, 20.55it/s, loss=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Avg Loss: 2.6793\n",
      "I am learning to speak! Womet—you’rl broken awrething scream?..”\n",
      "\n",
      "In those lay a\n",
      "tran and cald up themselves, the question \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 6252/6252 [05:13<00:00, 19.97it/s, loss=1.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Avg Loss: 2.6090\n",
      "I am learning to speak!” which the doors with the other, as in the exactly\n",
      "proud\n",
      "sciencity round Alexander\n",
      "met him as the p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 6252/6252 [04:58<00:00, 20.98it/s, loss=1.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Avg Loss: 2.6068\n",
      "I am learning to speak! No one is quite.... Hvand was an old unimpose, conjusured.”\n",
      "\n",
      "“As alone know when the letter is that\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 6252/6252 [05:09<00:00, 20.19it/s, loss=1.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Avg Loss: 2.6069\n",
      "I am learning to speak! I must stir.”\n",
      "\n",
      "“She was quite belonged how how Seens of the Russians were finished. What Napoleon R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 6252/6252 [04:59<00:00, 20.88it/s, loss=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Avg Loss: 2.6068\n",
      "I am learning to speak! It’s with you? Plass wail my effect you when my pitent\n",
      "before the bad,” cried\n",
      "Denísov. At that powe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 6252/6252 [05:08<00:00, 20.29it/s, loss=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Avg Loss: 2.6068\n",
      "I am learning to speak! We’ll cause it seems to be use’s mystacter in the\n",
      "rose,” he added, had confisured for glone conside\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 6252/6252 [05:00<00:00, 20.79it/s, loss=1.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Avg Loss: 2.6067\n",
      "I am learning to speak! What is?” said very and conquerwing abreading\n",
      "his own daughter.\n",
      "\n",
      "“Toifil, you know that’s intate\n",
      "I \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 6252/6252 [05:05<00:00, 20.47it/s, loss=1.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Avg Loss: 2.6067\n",
      "I am learning to speak! Why; have you like frame an. Had left Mérya Michaul Here decided to do now I see on the little arti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 6252/6252 [05:01<00:00, 20.74it/s, loss=1.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Avg Loss: 2.6067\n",
      "I am learning to speak! Grand is guilty....”\n",
      "\n",
      "“Oh!” he said to understand with her’s began do all before on the tears.\n",
      "\n",
      "“I \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 6252/6252 [05:05<00:00, 20.45it/s, loss=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Avg Loss: 2.6068\n",
      "I am learning to speak! An artions pressed, your case.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CHAPTER VIII\n",
      "\n",
      "He was behand and bulded here\n",
      "so her to the possi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 6252/6252 [05:02<00:00, 20.70it/s, loss=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Avg Loss: 2.6067\n",
      "I am learning to speak! Petersburg...”\n",
      "\n",
      "He did not did not bonead which had once could my difficult expression of the laws \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 6252/6252 [05:03<00:00, 20.62it/s, loss=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Avg Loss: 2.6067\n",
      "I am learning to speak! I know the Fulder,\n",
      "I are you done a feeling youth,” said the onficement ragh father, prisoning, and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 6252/6252 [05:01<00:00, 20.74it/s, loss=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Avg Loss: 2.6068\n",
      "I am learning to speak!” asked Pétya now as if waiting past the first ordinary\n",
      "confusions, already said with the street, sm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 6252/6252 [05:01<00:00, 20.74it/s, loss=1.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Avg Loss: 2.6068\n",
      "I am learning to speak!”\n",
      "\n",
      "“But she won’t now have it enjoyment—fornure their accustinate!”\n",
      "\n",
      "Andrew knew them held in which \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 6252/6252 [05:03<00:00, 20.58it/s, loss=1.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Avg Loss: 2.6068\n",
      "I am learning to speak!” he reproancing Dólokhov’s company.\n",
      "\n",
      "“And you have chelt this, would be asked our mensalle,” though\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 6252/6252 [04:59<00:00, 20.85it/s, loss=1.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Avg Loss: 2.6068\n",
      "I am learning to speak!” answered Pierre.\n",
      "\n",
      "At that\n",
      "had even fished to him, with at a little destricied him of his little an\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 6252/6252 [05:07<00:00, 20.35it/s, loss=1.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Avg Loss: 2.6068\n",
      "I am learning to speak!”\n",
      "\n",
      "Anna Pávlov, laughing was eachlifition on the Petroos, and Vinílirs with several table,\n",
      "who thoug\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 6252/6252 [04:55<00:00, 21.13it/s, loss=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Avg Loss: 2.6068\n",
      "I am learning to speak!” whispered, they did not what the fiftyout Et her hand this\n",
      "heed, is gave.”\n",
      "\n",
      "“Well, back Ilyín gay \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 6252/6252 [05:09<00:00, 20.17it/s, loss=1.29]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Avg Loss: 2.6068\n",
      "I am learning to speak!”\n",
      "\n",
      "“Why, I’ve\n",
      "was in bguoth to them?” And\n",
      "quite time he did he not be happening in the sound of\n",
      "in h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Lower complexity params\n",
    "seq_len = 100\n",
    "batch_size = 512\n",
    "epochs = 20\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "embed_size = 64\n",
    "\n",
    "dataset = CharDataset(text, seq_len, char2idx)\n",
    "dataloader2 = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model2 = CharLSTM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=7e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "for inputs, targets in tqdm(dataloader, desc=\"Warmup Batch\"):\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    with torch.no_grad():  # Add this - prevents gradient computation\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            log_probs, _ = model2(inputs)\n",
    "            loss = criterion(log_probs.view(-1, vocab_size), targets.view(-1))\n",
    "    \n",
    "    print(f\"Warmup loss: {loss.item()}\")\n",
    "    break \n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader2, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for inputs, targets in progress_bar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        h_0 = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "        c_0 = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "        hidden = (h_0, c_0)\n",
    "        \n",
    "        with torch.amp.autocast('cuda'):\n",
    "            log_probs, _ = model2(inputs, hidden)\n",
    "            loss = criterion(log_probs.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}\")\n",
    "    print(generate_text_during_train(model2, char2idx, idx2char, \"I am learning to speak!\", 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0e160017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for file in os.listdir('J.S.MILL/'):\n",
    "    file_path = os.path.join('J.S.MILL/', file)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        textJSM += f.read() + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c191b501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 3156593, Unique chars: 137\n"
     ]
    }
   ],
   "source": [
    "charsJSM = sorted(set(textJSM))\n",
    "vocab_sizeJSM = len(charsJSM)\n",
    "char2idxJSM = { ch:i for i,ch in enumerate(charsJSM) }\n",
    "idx2charJSM = { i:ch for i,ch in enumerate(charsJSM) }\n",
    "print(f\"Text length: {len(textJSM)}, Unique chars: {vocab_sizeJSM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "857fe148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params\n",
    "seq_len = 100\n",
    "batch_size = 512\n",
    "epochs = 5\n",
    "hidden_size = 1024\n",
    "num_layers = 4\n",
    "embed_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2dc4abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CharDataset(textJSM, seq_len, char2idxJSM)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CharLSTM(vocab_sizeJSM, embed_size, hidden_size, num_layers).to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=7e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e1c99414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup Batch:   0%|          | 0/6165 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup loss: 4.918237209320068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "for inputs, targets in tqdm(dataloader, desc=\"Warmup Batch\"):\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    with torch.no_grad():  # Add this - prevents gradient computation\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            log_probs, _ = model(inputs)\n",
    "            loss = criterion(log_probs.view(-1, vocab_sizeJSM), targets.view(-1))\n",
    "    \n",
    "    print(f\"Warmup loss: {loss.item()}\")\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aa14f665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am learning to speak!nro ldbc c\n",
      "prTt tndees hieemco re tthhrra,oe reosd,l yorvln eaan8sghobtoinhnoroteatvitocsmt;f,SrmIrf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 6165/6165 [48:26<00:00,  2.12it/s, loss=0.778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Avg Loss: 1.1225\n",
      "I am learning to speak! Never he feels himself\n",
      "support the liberty of those who possess, and improningencially, party\n",
      "more \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 6165/6165 [47:30<00:00,  2.16it/s, loss=0.557]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Avg Loss: 0.6448\n",
      "I am learning to speak! on\n",
      "the other hand, let me, or still more important fact, on the other hand,\n",
      "because they do not req\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 6165/6165 [46:52<00:00,  2.19it/s, loss=0.544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Avg Loss: 0.5466\n",
      "I am learning to speak! in such a country, and is a most important\n",
      "element in the act of Justice, which entitles he sees wr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 6165/6165 [46:50<00:00,  2.19it/s, loss=0.556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Avg Loss: 0.5466\n",
      "I am learning to speak! on the contrary, all\n",
      "despotism has to be recognized, and is carried into effect in the middle ages,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 6165/6165 [46:50<00:00,  2.19it/s, loss=0.547]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Avg Loss: 0.5466\n",
      "I am learning to speak! from these exertions, in this country, does not\n",
      "fall in any time more than high plate; and the more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_text_during_train(model, char2idxJSM, idx2charJSM, \"I am learning to speak!\",100))\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for inputs, targets in progress_bar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        h_0 = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "        c_0 = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "        hidden = (h_0, c_0)\n",
    "        \n",
    "        with torch.amp.autocast('cuda'):\n",
    "            log_probs, _ = model(inputs, hidden)\n",
    "            loss = criterion(log_probs.view(-1, vocab_sizeJSM), targets.view(-1))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}\")\n",
    "    print(generate_text_during_train(model, char2idxJSM, idx2charJSM, \"I am learning to speak!\",100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "86c39fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_str, length=200):\n",
    "    model.eval()\n",
    "    input_idxs = [char2idxJSM[ch] for ch in start_str]\n",
    "    input_tensor = torch.tensor(input_idxs, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    hidden = None\n",
    "    output_text = start_str\n",
    "    for _ in range(length):\n",
    "        out, hidden = model(input_tensor, hidden)\n",
    "        # Take the last timestep's output\n",
    "        last_logits = out[0, -1, :].cpu().detach().numpy()\n",
    "        # Sample from the softmax distribution\n",
    "        probs = np.exp(last_logits - np.max(last_logits))\n",
    "        probs /= probs.sum()\n",
    "        next_idx = np.random.choice(range(vocab_sizeJSM), p=probs)\n",
    "        output_text += idx2charJSM[next_idx]\n",
    "        input_tensor = torch.tensor([[next_idx]], dtype=torch.long, device=device)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aa5bff79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated text:\n",
      " My opinion on cats is a true belief that Germany remain\n",
      "under the study of equality. This remarkable condition of society, it is\n",
      "because it is a selfish object; but in all things of which the qualities\n",
      "established by a person of men is good at all; or, at all events, it\n",
      "does not, but has probably drawn not only to indefvenifely like the\n",
      "    character of the one as before. In the cases we may prevent the\n",
      "    chart, and in other ways, incident to it, and our disapprobation alone.\n",
      "    Indirect taxes are:\n",
      "On Secondly, t\n"
     ]
    }
   ],
   "source": [
    "seed = \"My opinion on cats is\"\n",
    "sample_text = generate_text(model, seed, length=500)\n",
    "print(\"Sample generated text:\\n\", sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "92bed203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup Batch:   0%|          | 0/12330 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup Batch:   0%|          | 0/12330 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup loss: 4.939479827880859\n",
      "I am learning to speak!ηw2p%Bxηο]τDιëτ¼æ;bR1d7vKά=SλJO!kUL&•$kV3χCBi2YFôZ?™B‘afκH*?’’(ô;uςHτv(σ5“D½JT$fzd¾M;-â&—aφῆxkHN™ι§e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 12330/12330 [14:01<00:00, 14.65it/s, loss=1.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Avg Loss: 1.5707\n",
      "I am learning to speak! Admits\n",
      "with her thousand -power to produce alchadized the power of\n",
      "the same, that the supply of the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 12330/12330 [13:45<00:00, 14.93it/s, loss=1.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Avg Loss: 1.1351\n",
      "I am learning to speak! Where leading\n",
      "manufacturing constituency has adricially five and look out\n",
      "what is really requires. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 12330/12330 [13:50<00:00, 14.86it/s, loss=1.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Avg Loss: 1.1351\n",
      "I am learning to speak! Are call at answer to the\n",
      "public wider opportunity of thrown and five indication that the\n",
      "community\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 12330/12330 [14:12<00:00, 14.47it/s, loss=1.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Avg Loss: 1.1351\n",
      "I am learning to speak! These words\n",
      "    on wages), if prey modemate precisely iv to say for the stote,\n",
      "    and state that t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 12330/12330 [13:44<00:00, 14.96it/s, loss=1.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Avg Loss: 1.1351\n",
      "I am learning to speak!\n",
      "\n",
      "The power was the most motive in those sexs, and that, as it was\n",
      "not accepted) to all duty without\n"
     ]
    }
   ],
   "source": [
    "#Params - Decrease embedding and hidden dimensions but increase  the number of layers\n",
    "seq_len = 100\n",
    "batch_size = 256\n",
    "epochs = 5\n",
    "hidden_size = 256\n",
    "num_layers = 8\n",
    "embed_size = 128\n",
    "\n",
    "dataset = CharDataset(textJSM, seq_len, char2idxJSM)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CharLSTM(vocab_sizeJSM, embed_size, hidden_size, num_layers).to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=7e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "for inputs, targets in tqdm(dataloader, desc=\"Warmup Batch\"):\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    with torch.no_grad():  # Add this - prevents gradient computation\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            log_probs, _ = model(inputs)\n",
    "            loss = criterion(log_probs.view(-1, vocab_sizeJSM), targets.view(-1))\n",
    "    \n",
    "    print(f\"Warmup loss: {loss.item()}\")\n",
    "    break \n",
    "\n",
    "print(generate_text_during_train(model, char2idxJSM, idx2charJSM, \"I am learning to speak!\",100))\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for inputs, targets in progress_bar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        h_0 = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "        c_0 = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "        hidden = (h_0, c_0)\n",
    "        \n",
    "        with torch.amp.autocast('cuda'):\n",
    "            log_probs, _ = model(inputs, hidden)\n",
    "            loss = criterion(log_probs.view(-1, vocab_sizeJSM), targets.view(-1))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}\")\n",
    "    print(generate_text_during_train(model, char2idxJSM, idx2charJSM, \"I am learning to speak!\",100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0589b2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated text:\n",
      " My opinion on cats is an equality to their\n",
      "spirits, and silver, except both on the amount, not so that no\n",
      "notions of the produce of banks, and he has\n",
      "avrigured to persons.\n",
      "\n",
      "I come to account the same trantasion).\n",
      "\n",
      "The upon Christianity does not enable connection, you will acfuse the principle of\n",
      "laborers owning this employed foreign metallic\n",
      "upward to its acts, and the position of\n",
      "the seeming as the exchange is foreign mode of\n",
      "greatest absolute waranties a subject, however, no\n",
      "man of the total selfishfores, and affe\n"
     ]
    }
   ],
   "source": [
    "seed = \"My opinion on cats is\"\n",
    "sample_text = generate_text(model, seed, length=500)\n",
    "print(\"Sample generated text:\\n\", sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d06643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4275ff6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
